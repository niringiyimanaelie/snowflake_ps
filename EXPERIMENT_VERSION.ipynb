{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from snowflake.snowpark.functions import(\n",
    "col, count, avg, max as max_, min as min_, dateadd, current_date, lit, sum as sum_, \n",
    "coalesce, datediff, any_value, when, array_unique_agg, array_size, regexp_replace, iff, nullifzero\n",
    ")\n",
    "from snowflake.ml.feature_store import FeatureStore, Entity, FeatureView, CreationMode\n",
    "\n",
    "# We can also use Snowpark for our analyses!\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55dea84-77bf-44eb-a70d-a9de85fad360",
   "metadata": {
    "collapsed": false,
    "name": "cell3"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65d476f-3aeb-4874-92e6-64ff055644ae",
   "metadata": {
    "collapsed": false,
    "name": "cell5"
   },
   "source": [
    "#### Call quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f5692-ed59-4fd4-a7dc-c63741424c4d",
   "metadata": {
    "language": "python",
    "name": "cell47"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc2cdd5-185b-4d2b-89e7-b04915f0e6e2",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": [
    "user_features_call_quality_df = session.sql(\"\"\"\n",
    "SELECT \n",
    "    USER_ID_HEX,\n",
    "    CAST(COUNT_IF(COALESCE(num_bad_mos_periods, 0) > 0) AS FLOAT) AS calls_with_bad_mos,\n",
    "    CAST(AVG(computed_mos) AS FLOAT) AS average_mos,\n",
    "    CAST(MAX(RTP_SETUP_TIME) AS FLOAT) AS max_rtp_setup_time,\n",
    "    CAST(COUNT_IF(CASE WHEN call_date >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN COALESCE(num_bad_mos_periods, 0) > 0 ELSE FALSE END) AS FLOAT) AS calls_with_bad_mos_7d,\n",
    "    CAST(AVG(CASE WHEN call_date >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN computed_mos END) AS FLOAT) AS average_mos_7d,\n",
    "    CAST(MAX(CASE WHEN call_date >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN RTP_SETUP_TIME END) AS FLOAT) AS max_rtp_setup_time_7d\n",
    "FROM dev.public.legacy_call_end\n",
    "WHERE call_date <= CURRENT_DATE()\n",
    "    AND USER_ID_HEX != '000-00-000-000000000'\n",
    "GROUP BY USER_ID_HEX\n",
    "\"\"\")\n",
    "user_features_call_quality_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603cdb18-465d-4929-ba22-6e388e619799",
   "metadata": {
    "collapsed": false,
    "name": "cell7"
   },
   "source": [
    "#### Call rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "user_features_call_rating_df =session.sql(\"\"\"\n",
    "SELECT \n",
    "    user_id_hex,\n",
    "    CAST(COUNT(call_rating) AS FLOAT) AS call_rating_count,\n",
    "    CAST(AVG(call_rating) AS FLOAT) AS avg_call_rating,\n",
    "    CAST(MAX(call_rating) AS FLOAT) AS max_call_rating,\n",
    "    CAST(MIN(call_rating) AS FLOAT) AS min_call_rating,\n",
    "    CAST(COUNT(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN call_rating END) AS FLOAT) AS call_rating_count_7d,\n",
    "    CAST(AVG(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN call_rating END) AS FLOAT) AS avg_call_rating_7d,\n",
    "    CAST(MAX(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN call_rating END) AS FLOAT) AS max_call_rating_7d,\n",
    "    CAST(MIN(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN call_rating END) AS FLOAT) AS min_call_rating_7d\n",
    "FROM dev.public.call_ratings_combined_sources\n",
    "WHERE date_utc <= CURRENT_DATE()\n",
    "    AND call_rating > 0\n",
    "    AND user_id_hex != '000-00-000-000000000'\n",
    "GROUP BY user_id_hex\n",
    "\"\"\")\n",
    "user_features_call_rating_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9686b061-7650-4ff3-945b-b8c0c6bd13b2",
   "metadata": {
    "collapsed": false,
    "name": "cell9"
   },
   "source": [
    "#### Data usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9567f26-69ea-4169-8767-f404b96227f3",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": [
    "user_features_data_usage_df = session.sql(\"\"\"\n",
    "SELECT \n",
    "    up.user_id_hex,\n",
    "    SUM(c.mb_usage) AS data_usage_mb,\n",
    "    SUM(CASE WHEN c.date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN c.mb_usage ELSE 0 END) AS data_usage_mb_7d\n",
    "FROM dev.public.cost_user_daily_tmobile_cost c\n",
    "JOIN dev.public.user_profiles up ON c.username = up.latest_username\n",
    "WHERE c.date_utc <= CURRENT_DATE()\n",
    "GROUP BY up.user_id_hex\n",
    "\"\"\")\n",
    "user_features_data_usage_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3cf53-b819-4430-981b-5bb2368e6449",
   "metadata": {
    "collapsed": false,
    "name": "cell4"
   },
   "source": [
    "#### Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aebd30-27bc-4e89-979e-597dc934e1e0",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": [
    "user_features_sessions_df = session.sql(\"\"\"\n",
    "SELECT \n",
    "    up.user_id_hex,\n",
    "    CAST(SUM(m.time_in_app_mins_per_day) AS FLOAT) AS time_in_app_mins,\n",
    "    CAST(DATEDIFF(day, ANY_VALUE(up.registered_at), CURRENT_DATE()) AS FLOAT) AS tenure_days,\n",
    "    CAST(SUM(m.num_sessions) AS FLOAT) AS session_count,\n",
    "    CAST(SUM(CASE WHEN m.date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN m.time_in_app_mins_per_day ELSE 0 END) AS FLOAT) AS time_in_app_mins_7d,\n",
    "    CAST(SUM(CASE WHEN m.date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN m.num_sessions ELSE 0 END) AS FLOAT) AS session_count_7d\n",
    "FROM dev.public.metrics_daily_userlevel_app_time_sessions m\n",
    "JOIN dev.public.user_profiles up ON m.username = up.latest_username\n",
    "WHERE m.date_utc <= CURRENT_DATE()\n",
    "GROUP BY up.user_id_hex\n",
    "\"\"\")\n",
    "user_features_sessions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e6855-47b1-45ce-813b-9039800c2a19",
   "metadata": {
    "collapsed": false,
    "name": "cell12"
   },
   "source": [
    "#### NPS ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c2997-2d0f-4ea1-90dd-8a3d2b96ffc2",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": [
    "user_features_nps_rating_df = session.sql(\"\"\"\n",
    "SELECT \n",
    "    user_id_hex,\n",
    "    CAST(COUNT(*) AS FLOAT) AS nps_count,\n",
    "    CAST(AVG(score) AS FLOAT) AS nps_avg_rating,\n",
    "    CAST(MAX(score) AS FLOAT) AS nps_max_rating,\n",
    "    CAST(COUNT(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN 1 END) AS FLOAT) AS nps_count_7d,\n",
    "    CAST(AVG(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN score END) AS FLOAT) AS nps_avg_rating_7d,\n",
    "    CAST(MAX(CASE WHEN date_utc >= CURRENT_DATE() - INTERVAL '7 DAYS' THEN score END) AS FLOAT) AS nps_max_rating_7d\n",
    "FROM dev.public.nps_combined_sources\n",
    "WHERE date_utc <= CURRENT_DATE()\n",
    "    AND user_id_hex != '000-00-000-000000000'\n",
    "GROUP BY user_id_hex\n",
    "\"\"\")\n",
    "user_features_nps_rating_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6875e5-b933-4af5-9169-85bc2f38e0a6",
   "metadata": {
    "collapsed": false,
    "name": "cell15"
   },
   "source": [
    "## Feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662eea8d-9482-43f3-bde1-b936d97519fc",
   "metadata": {
    "collapsed": false,
    "name": "cell31"
   },
   "source": [
    "#### Create FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94842b07-e510-421c-b09e-ca26a3b0313a",
   "metadata": {
    "language": "python",
    "name": "cell32"
   },
   "outputs": [],
   "source": [
    "# fs = FeatureStore(\n",
    "#     session=session,\n",
    "#     database=\"dev\",\n",
    "#     name=\"user_activity_feature_store\",\n",
    "#     default_warehouse=\"ds_wh_medium\",\n",
    "#     creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048df28-e948-4df3-ab01-3c100efbb755",
   "metadata": {
    "collapsed": false,
    "name": "cell16"
   },
   "source": [
    "#### Connect to FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0f328d-8589-49d1-88f8-68cbcdfc0ff3",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session,\n",
    "    database=\"dev\",\n",
    "    name=\"user_activity_feature_store\",\n",
    "    default_warehouse=\"ds_wh_medium\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d219cd46-8aa1-4387-900f-bd76d6fa9e08",
   "metadata": {
    "collapsed": false,
    "name": "cell33"
   },
   "source": [
    "#### Create and register entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42a7fae-1e8c-406a-ae85-a30e4b651b7d",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": [
    "# entity = Entity(\n",
    "#     name=\"user\",\n",
    "#     join_keys=[\"user_id_hex\"],\n",
    "#     desc=\"user entity\"\n",
    "# )\n",
    "# fs.register_entity(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5950726e-a5a8-4194-b8b6-f9987b91c5fd",
   "metadata": {
    "collapsed": false,
    "name": "cell19"
   },
   "source": [
    "#### Get existing entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e164b8b-2a49-4c4c-b7b4-10b9829c4bfb",
   "metadata": {
    "language": "python",
    "name": "cell17"
   },
   "outputs": [],
   "source": [
    "entity = fs.get_entity(\"user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f338f0d5-dad2-4f9a-9d1b-834ff4bbec75",
   "metadata": {
    "collapsed": false,
    "name": "cell10"
   },
   "source": [
    "#### Create and register feature views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282b5aef-1c20-4436-8801-cf626b22704d",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": [
    "# Call quality\n",
    "user_features_call_quality_fv = FeatureView(\n",
    "    name=\"user_features_call_quality\",\n",
    "    entities=[entity],\n",
    "    feature_df=user_features_call_quality_df,\n",
    "    refresh_freq=\"24 hours\",\n",
    "    desc=\"features about user call quality\"\n",
    ")\n",
    "\n",
    "fs.register_feature_view(\n",
    "    feature_view=user_features_call_quality_fv,\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f6ba8d-04e8-4428-83e5-8d07f6773737",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": [
    "# Call rating\n",
    "user_features_call_rating_fv = FeatureView(\n",
    "    name=\"user_features_call_rating\",\n",
    "    entities=[entity],\n",
    "    feature_df=user_features_call_rating_df,\n",
    "    refresh_freq=\"24 hours\",\n",
    "    desc=\"features about user call rating\"\n",
    ")\n",
    "\n",
    "fs.register_feature_view(\n",
    "    feature_view=user_features_call_rating_fv,\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e47333-ef0e-4cb1-8a79-52553993ed8f",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": [
    "# Data usage\n",
    "user_features_data_usage_fv = FeatureView(\n",
    "    name=\"user_features_data_usage\",\n",
    "    entities=[entity],\n",
    "    feature_df=user_features_data_usage_df,\n",
    "    refresh_freq=\"24 hours\",\n",
    "    desc=\"features about user data usage\"\n",
    ")\n",
    "\n",
    "fs.register_feature_view(\n",
    "    feature_view=user_features_data_usage_fv,\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79305644-899b-48f9-af02-d86f375ad818",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": [
    "# User sessions\n",
    "user_features_sessions_fv = FeatureView(\n",
    "    name=\"user_features_sessions\",\n",
    "    entities=[entity],\n",
    "    feature_df=user_features_sessions_df,\n",
    "    refresh_freq=\"24 hours\",\n",
    "    desc=\"features about user sessions\"\n",
    ")\n",
    "\n",
    "fs.register_feature_view(\n",
    "    feature_view=user_features_sessions_fv,\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5da96-5362-4786-a64a-9cb8a24cb8af",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": [
    "# NPS Rating\n",
    "user_features_nps_rating_fv = FeatureView(\n",
    "    name=\"user_features_nps_rating\",\n",
    "    entities=[entity],\n",
    "    feature_df=user_features_nps_rating_df,\n",
    "    refresh_freq=\"24 hours\",\n",
    "    desc=\"features about user NPS Rating\"\n",
    ")\n",
    "\n",
    "fs.register_feature_view(\n",
    "    feature_view=user_features_nps_rating_fv,\n",
    "    version=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc3bc5-4251-487b-9278-9e0dc6539671",
   "metadata": {
    "collapsed": false,
    "name": "cell24"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db6d4d-96ea-48fc-b337-f989d101864e",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from snowflake.ml.experiment import ExperimentTracking\n",
    "\n",
    "exp = ExperimentTracking(session=session)\n",
    "exp.set_experiment(\"Baseline_user_activity_forecastint_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b737f6-fe67-4a1e-9778-b968077b50a7",
   "metadata": {
    "collapsed": false,
    "name": "cell27"
   },
   "source": [
    "#### Spine df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ee0b5-9169-4d08-94c5-74ae3032ac24",
   "metadata": {
    "language": "python",
    "name": "cell28"
   },
   "outputs": [],
   "source": [
    "spine_df= session.sql(\"\"\"\n",
    "SELECT\n",
    "    up.user_id_hex,\n",
    "    sum(iff(m.time_in_app_mins_per_day > 1, 1, 0)) as active_days_in_week\n",
    "FROM dev.public.metrics_daily_userlevel_app_time_sessions m\n",
    "JOIN dev.public.user_profiles up ON m.username = up.latest_username\n",
    "WHERE m.date_utc >= dateadd('day', -14, current_date())\n",
    "GROUP by up.user_id_hex\n",
    "\"\"\")\n",
    "spine_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21daba-2cba-4ee1-9bcd-9e2f566f9b18",
   "metadata": {
    "collapsed": false,
    "name": "cell30"
   },
   "source": [
    "#### Get training dataset from FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a56c7b-d241-408f-a6d9-855f76a16219",
   "metadata": {
    "language": "python",
    "name": "cell29"
   },
   "outputs": [],
   "source": [
    "user_features_call_quality=fs.get_feature_view(name=\"user_features_call_quality\", version=\"1\")\n",
    "user_features_call_rating=fs.get_feature_view(name=\"user_features_call_rating\", version=\"1\")\n",
    "\n",
    "df = fs.generate_training_set(\n",
    "    spine_df=spine_df,\n",
    "    features=[user_features_call_quality, user_features_call_rating],\n",
    "    spine_label_cols=\"active_days_in_week\"\n",
    ")\n",
    "df = df.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53813030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot variable distributions\n",
    "def plot_variable_distributions(dataframe):\n",
    "    \"\"\"Plot histograms for all numeric features\"\"\"\n",
    "    # Get numeric columns\n",
    "    numeric_columns = dataframe.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    # Calculate grid size\n",
    "    num_features = len(numeric_columns)\n",
    "    num_cols = 3\n",
    "    num_rows = (num_features + num_cols - 1) // num_cols\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot each feature\n",
    "    for i, column_name in enumerate(numeric_columns):\n",
    "        axes[i].hist(dataframe[column_name].dropna(), bins=30, \n",
    "                    color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(column_name, fontsize=10)\n",
    "        axes[i].set_xlabel(column_name)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(num_features, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Variable Distributions\", fontsize=15, y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot distributions\n",
    "plot_variable_distributions(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ada5e",
   "metadata": {},
   "source": [
    "#### Feature Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot correlation matrix\n",
    "def plot_correlation_matrix(dataframe):\n",
    "    \"\"\"Plot heatmap showing correlations between features\"\"\"\n",
    "    # Select only numeric columns\n",
    "    numeric_data = dataframe.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = numeric_data.corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Matrix', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show correlation with target variable\n",
    "    print(\"\\nCorrelation with Target (ACTIVE_DAYS_IN_WEEK):\")\n",
    "    print(\"=\" * 60)\n",
    "    target_correlation = correlation_matrix['ACTIVE_DAYS_IN_WEEK'].sort_values(ascending=False)\n",
    "    for feature, corr_value in target_correlation.items():\n",
    "        print(f\"{feature}: {corr_value:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Call the function to plot correlation matrix\n",
    "plot_correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30032aa5",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure pandas to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Function to explore the dataset\n",
    "def explore_data(dataframe):\n",
    "    \"\"\"Display basic information about the dataset\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET EXPLORATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    print(dataframe.head())\n",
    "    \n",
    "    print(\"\\nDataset Shape:\")\n",
    "    print(f\"Rows: {dataframe.shape[0]}, Columns: {dataframe.shape[1]}\")\n",
    "    \n",
    "    print(\"\\nData Types of Each Column:\")\n",
    "    print(dataframe.dtypes)\n",
    "    \n",
    "    print(\"\\nStatistics of the Numerical Columns:\")\n",
    "    print(dataframe.describe())\n",
    "    \n",
    "    print(\"\\nMissing Values in Each Column:\")\n",
    "    print(dataframe.isnull().sum())\n",
    "    \n",
    "    print(\"\\nTarget Variable (ACTIVE_DAYS_IN_WEEK) Distribution:\")\n",
    "    print(dataframe['ACTIVE_DAYS_IN_WEEK'].describe())\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Call the function to explore data\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f6092",
   "metadata": {},
   "source": [
    "#### Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28325e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a02ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = df.drop(['USER_ID_HEX', 'ACTIVE_DAYS_IN_WEEK'], axis=1)\n",
    "y = df['ACTIVE_DAYS_IN_WEEK']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INITIAL FEATURE SET\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total features: {X.shape[1]}\")\n",
    "print(f\"Total samples: {X.shape[0]}\")\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, feature_name in enumerate(X.columns, 1):\n",
    "    print(f\"  {i}. {feature_name}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168c6b47",
   "metadata": {},
   "source": [
    "#### Prepare Features and Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53397559-0294-42c5-b841-6415c6b1db0c",
   "metadata": {
    "collapsed": false,
    "name": "cell25"
   },
   "source": [
    "#### Split training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042cafe-6c83-4e56-9fdd-ad724e72c8e2",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "print(\"=\" * 60)\n",
    "print(\"SPLITTING DATA INTO TRAIN AND TEST SETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples, {X_test.shape[1]} features\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final feature set summary after feature selection\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL FEATURE SET AFTER SELECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total features selected: {X.shape[1]}\")\n",
    "print(f\"\\nSelected features for modeling:\")\n",
    "for i, feature_name in enumerate(X.columns, 1):\n",
    "    print(f\"  {i}. {feature_name}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13012e43",
   "metadata": {},
   "source": [
    "#### Update X with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Function to remove low variance features\n",
    "def remove_low_variance_features(dataframe, variance_threshold=0.01):\n",
    "    \"\"\"Remove features that have very low variance (almost constant values)\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"REMOVING LOW VARIANCE FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create variance selector\n",
    "    variance_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "    variance_selector.fit(dataframe)\n",
    "    \n",
    "    # Get feature names that pass the threshold\n",
    "    selected_features = dataframe.columns[variance_selector.get_support()].tolist()\n",
    "    removed_features = dataframe.columns[~variance_selector.get_support()].tolist()\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Variance threshold: {variance_threshold}\")\n",
    "    print(f\"Total features to remove: {len(removed_features)}\")\n",
    "    \n",
    "    if removed_features:\n",
    "        print(\"\\nFeatures being removed (low variance):\")\n",
    "        for feature in removed_features:\n",
    "            feature_variance = dataframe[feature].var()\n",
    "            print(f\"  - {feature} (variance: {feature_variance:.6f})\")\n",
    "    else:\n",
    "        print(\"No low variance features found!\")\n",
    "    \n",
    "    # Create dataframe with selected features\n",
    "    dataframe_after_removal = dataframe[selected_features]\n",
    "    \n",
    "    print(f\"\\nFeatures before: {dataframe.shape[1]}\")\n",
    "    print(f\"Features after: {dataframe_after_removal.shape[1]}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return dataframe_after_removal, removed_features\n",
    "\n",
    "# Step 2: Remove low variance features from X\n",
    "X = remove_low_variance_features(X, variance_threshold=0.01)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9129e",
   "metadata": {},
   "source": [
    "#### Variance Threshold Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1aec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove highly correlated features\n",
    "def remove_correlated_features(dataframe, correlation_threshold=0.9):\n",
    "    \"\"\"Remove features that are highly correlated with each other\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"REMOVING HIGHLY CORRELATED FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = dataframe.corr().abs()\n",
    "    \n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper_triangle = correlation_matrix.where(\n",
    "        np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find features with correlation greater than threshold\n",
    "    features_to_drop = [column for column in upper_triangle.columns \n",
    "                       if any(upper_triangle[column] > correlation_threshold)]\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Correlation threshold: {correlation_threshold}\")\n",
    "    print(f\"Total features to remove: {len(features_to_drop)}\")\n",
    "    \n",
    "    if features_to_drop:\n",
    "        print(\"\\nFeatures being removed:\")\n",
    "        for feature in features_to_drop:\n",
    "            print(f\"  - {feature}\")\n",
    "    else:\n",
    "        print(\"No highly correlated features found!\")\n",
    "    \n",
    "    # Remove correlated features\n",
    "    dataframe_after_removal = dataframe.drop(columns=features_to_drop)\n",
    "    \n",
    "    print(f\"\\nFeatures before: {dataframe.shape[1]}\")\n",
    "    print(f\"Features after: {dataframe_after_removal.shape[1]}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return dataframe_after_removal, features_to_drop\n",
    "\n",
    "# Step 1: Remove highly correlated features from X\n",
    "X = remove_correlated_features(X, correlation_threshold=0.9)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598d20e",
   "metadata": {},
   "source": [
    "#### Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60257f89",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a2d1c-fac9-4cfc-94e3-6144b4952cb4",
   "metadata": {
    "collapsed": false,
    "name": "cell39"
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9237d-9e9a-4aba-8cc3-40b09502b160",
   "metadata": {
    "language": "python",
    "name": "cell40"
   },
   "outputs": [],
   "source": [
    "numerical_cols = X.select_dtypes(include=['number']).columns\n",
    "preprocess=ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), numerical_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8f3a39-cc00-4f47-b589-15d377f55bdf",
   "metadata": {
    "collapsed": false,
    "name": "cell36"
   },
   "source": [
    "#### Base training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ced4e3-0af4-4579-91f6-e6ae1444a291",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": [
    "baseline_models = [\n",
    "    # Linear models\n",
    "    (\"LinearRegression\", LinearRegression(), {}),\n",
    "    (\"Ridge\", Ridge(), {'model__alpha': [0.1, 1.0, 10.0, 100.0]}),\n",
    "    (\"Lasso\", Lasso(), {'model__alpha': [0.1, 1.0, 10.0, 100.0]}),\n",
    "    \n",
    "    # Random Forest variations\n",
    "    (\"RandomForest\", RandomForestRegressor(), {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [5, 10, 15, None],\n",
    "        'model__min_samples_split': [2, 5, 10]\n",
    "    }),\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    (\"GradientBoosting\", GradientBoostingRegressor(), {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7]\n",
    "    }),\n",
    "    \n",
    "    # XGBoost\n",
    "    (\"XGBoost\", XGBRegressor(verbosity=0), {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7]\n",
    "    }),\n",
    "    \n",
    "    # LightGBM\n",
    "    (\"LightGBM\", LGBMRegressor(), {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 5, 7]\n",
    "    })\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d89592-9c54-4d96-9fd4-604725dcc519",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "cell38"
   },
   "outputs": [],
   "source": [
    "results=[]\n",
    "for name, model, param_grid in baseline_models:\n",
    "    print(f\"Training with RandomizedSearch: {name}\")\n",
    "\n",
    "    pipeline = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Use RandomizedSearchCV if parameters are provided\n",
    "    if param_grid:\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=10,\n",
    "            cv=3,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1,\n",
    "            random_state=42\n",
    "        )\n",
    "        random_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_pipeline = random_search.best_estimator_\n",
    "        best_params = random_search.best_params_\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        print(f\"{name} -> Best Params: {best_params}\")\n",
    "        print(f\"{name} -> Test MSE: {mse:.4f}\")\n",
    "        \n",
    "        results.append((name, mse, best_pipeline))\n",
    "        \n",
    "        # Log experiment with best parameters\n",
    "        with exp.start_run():\n",
    "            exp.log_metric(\"mse\", mse)\n",
    "            exp.log_param(\"model_type\", name)\n",
    "            for param_name, param_value in best_params.items():\n",
    "                exp.log_param(param_name, param_value)\n",
    "            exp.log_model(model=best_pipeline, model_name=f\"{name}_model\", sample_input_data=X_train.head())\n",
    "    else:\n",
    "        # Fit directly without RandomizedSearch for models with no hyperparameters\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        results.append((name, mse, pipeline))\n",
    "        print(f\"{name} -> Test MSE: {mse:.4f}\")\n",
    "        \n",
    "        # Log experiment\n",
    "        with exp.start_run():\n",
    "            exp.log_metric(\"mse\", mse)\n",
    "            exp.log_param(\"model_type\", name)\n",
    "            exp.log_model(model=pipeline, model_name=f\"{name}_model\", sample_input_data=X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60aee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final features for production based on SHAP importance\n",
    "num_production_features = 8  # Number of features to select for production\n",
    "\n",
    "production_features_df = shap_importance_df.copy()\n",
    "production_features_list = production_features_df.head(num_production_features)['feature'].tolist()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RECOMMENDED FEATURES FOR PRODUCTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSelected {num_production_features} most important features based on SHAP analysis:\")\n",
    "print()\n",
    "\n",
    "for i, feature_name in enumerate(production_features_list, 1):\n",
    "    shap_importance_value = production_features_df[\n",
    "        production_features_df['feature'] == feature_name\n",
    "    ]['shap_importance'].values[0]\n",
    "    print(f\"{i}. {feature_name}\")\n",
    "    print(f\"   SHAP Importance: {shap_importance_value:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original number of features: {len(X.columns)}\")\n",
    "print(f\"Recommended for production: {len(production_features_list)}\")\n",
    "print(f\"Feature reduction: {(1 - len(production_features_list)/len(X.columns))*100:.1f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a87cb",
   "metadata": {},
   "source": [
    "#### Final Production Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76e02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Waterfall Plot - explains a single prediction\n",
    "# Shows how each feature contributes to one specific prediction\n",
    "prediction_index = 0  # First prediction\n",
    "\n",
    "shap.waterfall_plot(\n",
    "    shap.Explanation(\n",
    "        values=shap_values[prediction_index], \n",
    "        base_values=shap_explainer.expected_value if hasattr(shap_explainer, 'expected_value') else 0,\n",
    "        data=X_test_preprocessed[prediction_index],\n",
    "        feature_names=X_test.columns.tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831f4f8",
   "metadata": {},
   "source": [
    "#### SHAP Waterfall Plot (Individual Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc9ba09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot - shows mean absolute SHAP values (global feature importance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, \n",
    "                  X_test.iloc[:len(shap_values)], \n",
    "                  feature_names=X_test.columns.tolist(),\n",
    "                  plot_type=\"bar\",\n",
    "                  show=False)\n",
    "plt.title(f'SHAP Feature Importance - {best_model_name}', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate mean absolute SHAP values for each feature\n",
    "mean_abs_shap_values = np.abs(shap_values).mean(axis=0)\n",
    "feature_names_list = X_test.columns.tolist()\n",
    "\n",
    "# Create dataframe with SHAP importance\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': feature_names_list,\n",
    "    'shap_importance': mean_abs_shap_values\n",
    "}).sort_values('shap_importance', ascending=False)\n",
    "\n",
    "print(\"\\nSHAP-based Feature Importance:\")\n",
    "print(\"=\" * 60)\n",
    "print(shap_importance_df)\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396de57c",
   "metadata": {},
   "source": [
    "#### SHAP Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot - shows feature importance and impact direction\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values, \n",
    "                  X_test.iloc[:len(shap_values)], \n",
    "                  feature_names=X_test.columns.tolist(),\n",
    "                  show=False)\n",
    "plt.title(f'SHAP Summary Plot - {best_model_name}', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989b6d2",
   "metadata": {},
   "source": [
    "#### SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8605b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Prepare data for SHAP analysis\n",
    "# Transform training and test data using the preprocessing pipeline\n",
    "X_train_preprocessed = best_model_pipeline.named_steps['preprocess'].transform(X_train)\n",
    "X_test_preprocessed = best_model_pipeline.named_steps['preprocess'].transform(X_test)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"SHAP ANALYSIS FOR {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create SHAP explainer based on model type\n",
    "if best_model_name in ['XGBoost', 'LightGBM', 'RandomForest', 'GradientBoosting']:\n",
    "    # Use TreeExplainer for tree-based models (faster)\n",
    "    print(\"Using TreeExplainer for tree-based model...\")\n",
    "    shap_explainer = shap.TreeExplainer(trained_model)\n",
    "    shap_values = shap_explainer.shap_values(X_test_preprocessed)\n",
    "    \n",
    "else:\n",
    "    # Use KernelExplainer for other models (slower but works for any model)\n",
    "    print(\"Using KernelExplainer for linear model...\")\n",
    "    \n",
    "    # Use smaller sample for faster computation\n",
    "    sample_size = min(100, len(X_train_preprocessed))\n",
    "    X_train_sample = X_train_preprocessed[:sample_size]\n",
    "    \n",
    "    shap_explainer = shap.KernelExplainer(trained_model.predict, X_train_sample)\n",
    "    \n",
    "    # Compute SHAP values for smaller test set\n",
    "    sample_test_size = min(50, len(X_test_preprocessed))\n",
    "    shap_values = shap_explainer.shap_values(X_test_preprocessed[:sample_test_size])\n",
    "    X_test_preprocessed = X_test_preprocessed[:sample_test_size]\n",
    "\n",
    "print(\"SHAP values computed successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb77336",
   "metadata": {},
   "source": [
    "#### SHAP Analysis for Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d177e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install SHAP\n",
    "# !pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f23f874",
   "metadata": {},
   "source": [
    "#### Install SHAP (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1a4295",
   "metadata": {},
   "source": [
    "## SHAP Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare feature importance across all tree-based models\n",
    "all_model_importances = {}\n",
    "\n",
    "# Extract feature importance from each model\n",
    "for model_name, model_mse, model_pipeline in results:\n",
    "    trained_model = model_pipeline.named_steps['model']\n",
    "    \n",
    "    if hasattr(trained_model, 'feature_importances_'):\n",
    "        feature_importance_values = trained_model.feature_importances_\n",
    "        feature_names_list = X_train.columns.tolist()\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names_list,\n",
    "            'importance': feature_importance_values\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        all_model_importances[model_name] = importance_df\n",
    "\n",
    "# Plot comparison if we have any tree-based models\n",
    "if all_model_importances:\n",
    "    num_models = len(all_model_importances)\n",
    "    fig, axes = plt.subplots(num_models, 1, figsize=(12, 5 * num_models))\n",
    "    \n",
    "    # Handle single model case\n",
    "    if num_models == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Plot each model's feature importance\n",
    "    for i, (model_name, importance_df) in enumerate(all_model_importances.items()):\n",
    "        top_10_features = importance_df.head(10)\n",
    "        axes[i].barh(top_10_features['feature'], top_10_features['importance'], color='skyblue')\n",
    "        axes[i].set_xlabel('Importance')\n",
    "        axes[i].set_title(f'Top 10 Features - {model_name}')\n",
    "        axes[i].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Feature importance comparison completed for all tree-based models\")\n",
    "else:\n",
    "    print(\"No tree-based models found for feature importance comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4b386",
   "metadata": {},
   "source": [
    "#### Feature Importance for All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb29ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model from results\n",
    "best_model_name = \"\"\n",
    "best_model_mse = float('inf')\n",
    "best_model_pipeline = None\n",
    "\n",
    "for model_name, model_mse, model_pipeline in results:\n",
    "    if model_mse < best_model_mse:\n",
    "        best_model_mse = model_mse\n",
    "        best_model_name = model_name\n",
    "        best_model_pipeline = model_pipeline\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model Name: {best_model_name}\")\n",
    "print(f\"Test MSE: {best_model_mse:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract the trained model from pipeline\n",
    "trained_model = best_model_pipeline.named_steps['model']\n",
    "\n",
    "# Check if model has feature importance\n",
    "if hasattr(trained_model, 'feature_importances_'):\n",
    "    # For tree-based models\n",
    "    feature_importance_values = trained_model.feature_importances_\n",
    "    feature_names_list = X_train.columns.tolist()\n",
    "    \n",
    "    # Create dataframe for feature importance\n",
    "    importance_dataframe = pd.DataFrame({\n",
    "        'feature': feature_names_list,\n",
    "        'importance': feature_importance_values\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(importance_dataframe)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_dataframe['feature'], importance_dataframe['importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select top features for production\n",
    "    top_n_features = 10\n",
    "    top_features_list = importance_dataframe.head(top_n_features)['feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nTop {top_n_features} Features Recommended for Production:\")\n",
    "    print(\"=\" * 60)\n",
    "    for i, feature_name in enumerate(top_features_list, 1):\n",
    "        importance_value = importance_dataframe[importance_dataframe['feature'] == feature_name]['importance'].values[0]\n",
    "        print(f\"{i}. {feature_name}: {importance_value:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "elif hasattr(trained_model, 'coef_'):\n",
    "    # For linear models\n",
    "    print(f\"\\n{best_model_name} is a linear model - using coefficients as importance\")\n",
    "    \n",
    "    coefficient_values = np.abs(trained_model.coef_)\n",
    "    feature_names_list = X_train.columns.tolist()\n",
    "    \n",
    "    importance_dataframe = pd.DataFrame({\n",
    "        'feature': feature_names_list,\n",
    "        'importance': coefficient_values\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance (based on absolute coefficients):\")\n",
    "    print(importance_dataframe)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_dataframe['feature'], importance_dataframe['importance'], color='skyblue')\n",
    "    plt.xlabel('|Coefficient|')\n",
    "    plt.title(f'Feature Importance - {best_model_name}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(f\"\\n{best_model_name} does not support feature importance extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba851dc9",
   "metadata": {},
   "source": [
    "#### Extract Feature Importance from Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed7770",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303597ae-4a03-4d0e-9cf7-d1cd96620912",
   "metadata": {
    "collapsed": false,
    "name": "cell41"
   },
   "source": [
    "## Model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93e01a-8bed-4bc6-8c49-05a0424e956e",
   "metadata": {
    "language": "python",
    "name": "cell46"
   },
   "outputs": [],
   "source": [
    "exp.list_artifacts(\"HOT_KIWI_4\", artifact_path=\"manifest.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13efa655-5d44-4dfb-bd22-96dd70903135",
   "metadata": {
    "language": "python",
    "name": "cell42"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.registry import Registry\n",
    "registry = Registry(session=session, database_name=\"dev\", schema_name=\"data_science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f89ad9-593f-4f49-b193-852fd22de4ea",
   "metadata": {
    "language": "python",
    "name": "cell44"
   },
   "outputs": [],
   "source": [
    "# # Log the model\n",
    "# model_version = registry.log_model(\n",
    "#     model=LinearRegression,\n",
    "#     model_name=\"base_linear_regression_model\",\n",
    "#     version_name=\"v1.0\",\n",
    "#     comment=\"Base version of the linear regression model\",\n",
    "#     sample_input_data=X.head()\n",
    "# )\n",
    "# print(f\"Model '{model_version.model_name}' version '{model_version.version_name}' registered successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fc0868-0467-4c37-91f7-b9407765adf5",
   "metadata": {
    "collapsed": false,
    "name": "cell43"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d00eb68-8ea1-4b9b-9646-eec8ba6f0a75",
   "metadata": {
    "language": "python",
    "name": "cell45"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "elie.niringiyim@textnow.com",
   "authorId": "522148860138",
   "authorName": "ELIE.NIRINGIYIM@TEXTNOW.COM",
   "lastEditTime": 1764735953621,
   "notebookId": "5kj7agku7vegpv2ackxz",
   "sessionId": "6ce57023-d619-45d7-aead-01ccbd08699f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
